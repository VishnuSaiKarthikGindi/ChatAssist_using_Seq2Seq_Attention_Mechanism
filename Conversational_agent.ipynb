{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d774c6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import re\n",
    "import numpy as np\n",
    "import spacy\n",
    "from torch.utils.data import random_split\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from torch.utils.data import DataLoader, Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "042d4bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5595ee5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3472f4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download with: python -m spacy download en_core_web_sm\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def preprocess_text(text):\n",
    "    \"\"\"\n",
    "    Preprocess the input text by performing operations such as lowercasing,\n",
    "    removing punctuation, and removing extra whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()  # Lowercase the text\n",
    "    text = re.sub(f\"[{re.escape(string.punctuation)}]\", \"\", text)  # Remove punctuation\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove extra spaces\n",
    "    text = text.strip()  # Remove leading and trailing spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a34cee1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Vocab:\n",
    "    def __init__(self, min_freq):\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {v: k for k, v in self.itos.items()}\n",
    "        self.min_freq = min_freq\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenize(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocab(self, sentences):\n",
    "        freqs = {}\n",
    "        idx = 4\n",
    "        for sentence in sentences:\n",
    "            for word in self.tokenize(sentence):\n",
    "                freqs[word] = freqs.get(word, 0) + 1\n",
    "                if freqs[word] == self.min_freq:\n",
    "                    self.stoi[word] = idx\n",
    "                    self.itos[idx] = word\n",
    "                    idx += 1\n",
    "\n",
    "    def numericalize(self, text):\n",
    "        tokenized_text = self.tokenizer_eng(text)\n",
    "        return [self.stoi[token] if token in self.stoi else self.stoi[\"<UNK>\"] for token in tokenized_text]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95859c9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QADataset(Dataset):\n",
    "    def __init__(self, qa_file, transform=None, freq_threshold=5):\n",
    "        self.transform = transform\n",
    "\n",
    "        with open(qa_file, 'r') as file:\n",
    "            lines = file.readlines()\n",
    "\n",
    "        self.questions = []\n",
    "        self.answers = []\n",
    "\n",
    "        for line in lines:\n",
    "            q, a = line.strip().split('\\t')\n",
    "            self.questions.append(q)\n",
    "            self.answers.append(a)\n",
    "\n",
    "        self.vocab = Vocab(freq_threshold)\n",
    "        self.vocab.build_vocabulary(self.questions + self.answers)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.questions)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        question = self.questions[index]\n",
    "        answer = self.answers[index]\n",
    "\n",
    "        numericalized_question = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_question += self.vocab.numericalize(question)\n",
    "        numericalized_question.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        numericalized_answer = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_answer += self.vocab.numericalize(answer)\n",
    "        numericalized_answer.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        return torch.tensor(numericalized_question), torch.tensor(numericalized_answer)\n",
    "    \n",
    "class BatchCollator:\n",
    "    def __init__(self, pad_idx):\n",
    "        self.pad_idx = pad_idx\n",
    "\n",
    "    def __call__(self, batch):\n",
    "        questions = [item[0] for item in batch]\n",
    "        answers = [item[1] for item in batch]\n",
    "        questions_padded = pad_sequence(questions, batch_first=False, padding_value=self.pad_idx)\n",
    "        answers_padded = pad_sequence(answers, batch_first=False, padding_value=self.pad_idx)\n",
    "        return questions_padded, answers_padded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56f591c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_data_loader(qa_file, batch_size=32, num_workers=2, shuffle=True, pin_memory=True, freq_threshold=5):\n",
    "    dataset = QADataset(qa_file, freq_threshold=freq_threshold)\n",
    "    pad_idx=dataset.vocab.stoi[\"<PAD>\"]\n",
    "    size = len(dataset)\n",
    "    val_size = int(0.1 * size)\n",
    "    train_size = dataset_size - val_size \n",
    "    train_dataset, val_dataset, test_dataset = random_split(dataset, [train_size, val_size])\n",
    "\n",
    "    train_loader = DataLoader(\n",
    "        dataset=train_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=shuffle,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=BatchCollator(pad_idx=pad_idx),\n",
    "    )\n",
    "    val_loader = DataLoader(\n",
    "        dataset=val_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=num_workers,\n",
    "        shuffle=False,\n",
    "        pin_memory=pin_memory,\n",
    "        collate_fn=BatchCollator(pad_idx=pad_idx),\n",
    "    )\n",
    "\n",
    "    return train_loader, val_loader, dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6629a96",
   "metadata": {},
   "outputs": [],
   "source": [
    "QA_file = \"Dataset.txt\" \n",
    "train_loader, val_loader, dataset = create_data_loader(QA_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97de55f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of Encoder and decoder architecture using the context vectors for attention mechanism\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, num_layers, dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(embed_size, hidden_size, num_layers, bidirectional=True, dropout=dropout)\n",
    "        self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        encoder_outputs, (hidden, cell) = self.lstm(x)\n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=1))\n",
    "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=1))\n",
    "        return encoder_outputs, hidden, cell\n",
    "    \n",
    "\"\"\"\n",
    "#Implementation using BERT for better semantic understanding\n",
    "class ContextualEncoder(nn.Module):\n",
    "    def __init__(self, hidden_size, num_layers, p):\n",
    "        super(ContextualEncoder, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.lstm = nn.LSTM(self.bert.config.hidden_size, hidden_size, num_layers, bidirectional=True, dropout=dropout)\n",
    "        #self.fc_hidden = nn.Linear(hidden_size * 2, hidden_size)\n",
    "        #self.fc_cell = nn.Linear(hidden_size * 2, hidden_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        with torch.no_grad():\n",
    "            outputs = self.bert(x)\n",
    "        encoder_outputs = outputs.last_hidden_state\n",
    "        hidden, cell = self.lstm(encoder_states)\n",
    "        hidden = self.fc_hidden(torch.cat((hidden[0:1], hidden[1:2]), dim=2))\n",
    "        cell = self.fc_cell(torch.cat((cell[0:1], cell[1:2]), dim=2))\n",
    "        return encoder_outputs, hidden, cell\n",
    "\"\"\"\n",
    "\n",
    "class Decoder(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_size, hidden_size, output_size, num_layers, dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_size)\n",
    "        self.lstm = nn.LSTM(hidden_size*2 + embed_size, hidden_size, num_layers, dropout=dropout)\n",
    "        self.attention = nn.Linear(hidden_size*3, 1)\n",
    "        self.fc_out = nn.Linear(hidden_size, output_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, encoder_outputs, hidden, cell):\n",
    "        x = x.unsqueeze(0)\n",
    "        embed = self.dropout(self.embedding(x))\n",
    "        sequence_length = encoder_outputs.shape[0]\n",
    "        h_reshaped = hidden.repeat(sequence_length, 1, 1)\n",
    "        energy = torch.tanh(self.attention(torch.cat((h_reshaped, encoder_outputs), dim=2)))\n",
    "        attention_weights = torch.softmax(energy, dim=0)\n",
    "        context_vector = torch.einsum(\"snk,snl->knl\", attention_weights, encoder_outputs)\n",
    "        lstm_input = torch.cat((context_vector, embed), dim=2)\n",
    "        outputs, (hidden, cell) = self.lstm(lstm_input, (hidden, cell))\n",
    "        predictions = self.fc_out(outputs).squeeze(0)\n",
    "        return predictions, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39258af1",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Implementation of Sequence to Sequence model using LSTM cells \n",
    "class Seq2Seq(nn.Module):\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super(Seq2Seq, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        batch_size = src.shape[1]\n",
    "        trg_len = trg.shape[0]\n",
    "        trg_vocab_size = len(dataset.vocab)\n",
    "\n",
    "        outputs = torch.zeros(trg_len, batch_size, trg_vocab_size).to(src.device)\n",
    "        encoder_outputs, hidden, cell = self.encoder(src)\n",
    "\n",
    "        x = trg[0]\n",
    "        for t in range(1, trg_len):\n",
    "            output, hidden, cell = self.decoder(x, encoder_outputs, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            best_guess = output.argmax(1)\n",
    "            x = trg[t] if torch.rand(1).item() < teacher_forcing_ratio else best_guess\n",
    "\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1a9dd8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training parameters\n",
    "num_epochs = 100\n",
    "learning_rate = 3e-4\n",
    "batch_size = 32\n",
    "\n",
    "# Model hyperparameters\n",
    "input_size_encoder = len(dataset.vocab)\n",
    "input_size_decoder = len(dataset.vocab)\n",
    "output_size = len(dataset.vocab)\n",
    "encoder_embedding_size = 300\n",
    "decoder_embedding_size = 300\n",
    "hidden_size = 1024\n",
    "num_layers = 1\n",
    "enc_dropout = 0.0\n",
    "dec_dropout = 0.0\n",
    "\n",
    "# Tensorboard for tracking loss\n",
    "writer = SummaryWriter(f\"runs/loss_plot\")\n",
    "step = 0\n",
    "\n",
    "# Define models\n",
    "encoder_net = Encoder(input_size_encoder, encoder_embedding_size, hidden_size, num_layers, enc_dropout).to(device)\n",
    "decoder_net = Decoder(output_size, decoder_embedding_size, hidden_size, num_layers, dec_dropout).to(device)\n",
    "model = Seq2Seq(encoder_net, decoder_net).to(device)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "pad_idx = dataset.vocab.stoi[\"<PAD>\"]\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "\n",
    "# Load model\n",
    "load_model = False\n",
    "save_model = False\n",
    "\n",
    "if load_model:\n",
    "    checkpoint = torch.load('seq2seq_model.pth')\n",
    "    encoder.load_state_dict(checkpoint['encoder_state_dict'])\n",
    "    decoder.load_state_dict(checkpoint['decoder_state_dict'])\n",
    "\n",
    "    encoder.eval()\n",
    "    decoder.eval()\n",
    "\n",
    "# Training loop\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"[Epoch {epoch + 1} / {num_epochs}]\")\n",
    "\n",
    "    model.train()\n",
    "    train_loss = 0\n",
    "    for batch_idx, (questions, answers) in enumerate(train_loader):\n",
    "        questions = questions.to(device)\n",
    "        answers = answers.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        output = model(questions, answers)\n",
    "\n",
    "        # Reshape output and target for loss computation\n",
    "        output = output[1:].reshape(-1, output.shape[2])\n",
    "        answers = answers[1:].reshape(-1)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss = criterion(output, answers)\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # Backward pass\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1)\n",
    "        optimizer.step()\n",
    "\n",
    "        # Track loss\n",
    "        writer.add_scalar(\"Training loss\", loss.item(), global_step=step)\n",
    "        step += 1\n",
    "\n",
    "    train_losses.append(train_loss / len(train_loader))\n",
    "    print(f\"Training loss: {train_loss / len(train_loader):.4f}\")\n",
    "\n",
    "    model.eval()\n",
    "    val_loss = 0\n",
    "    with torch.no_grad():\n",
    "        for questions, answers in val_loader:\n",
    "            questions = questions.to(device)\n",
    "            answers = answers.to(device)\n",
    "\n",
    "            output = model(questions, answers)\n",
    "\n",
    "            output = output[1:].reshape(-1, output.shape[2])\n",
    "            answers = answers[1:].reshape(-1)\n",
    "\n",
    "            loss = criterion(output, answers)\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    val_losses.append(val_loss / len(val_loader))\n",
    "    print(f\"Validation loss: {val_loss / len(val_loader):.4f}\")\n",
    "\n",
    "    # Save the models state_dicts\n",
    "    if save_model:\n",
    "        checkpoint = {\n",
    "                      'encoder_state_dict': encoder.state_dict(),\n",
    "                      'decoder_state_dict': decoder.state_dict(),\n",
    "                    }\n",
    "        torch.save(checkpoint, 'seq2seq_model.pth')\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(range(1, num_epochs + 1), train_losses, label=\"Training Loss\")\n",
    "plt.plot(range(1, num_epochs + 1), val_losses, label=\"Validation Loss\")\n",
    "plt.xlabel(\"Epochs\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.title(\"Training and Validation Loss\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e28399",
   "metadata": {},
   "outputs": [],
   "source": [
    "%tensorboard --logdir runs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e97fd86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Funtion to generate output greedily\n",
    "def decode_response(encoder, decoder, test_input, vocab):\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        encoder_states, hidden, cell = encoder(test_input)\n",
    "        target_seq = torch.tensor([vocab.stoi[\"<SOS>\"]]).unsqueeze(0).to(device)\n",
    "        decoded_sentence = ''\n",
    "        stop_condition = False\n",
    "        \n",
    "        while not stop_condition:\n",
    "            output, hidden, cell = decoder(target_seq, encoder_states, hidden, cell)\n",
    "            topi = output.argmax(1).item()\n",
    "            if topi == vocab.stoi[\"<EOS>\"] or len(decoded_sentence.split()) > 50:\n",
    "                stop_condition = True\n",
    "            else:\n",
    "                decoded_sentence += vocab.itos[topi] + ' '\n",
    "            target_seq = torch.tensor([topi]).unsqueeze(0).to(device)\n",
    "            \n",
    "    return decoded_sentence.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e1f2c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to generate output using beam search\n",
    "def decode_response_beam_search(encoder, decoder, input_seq, max_length, vocab, beam_width=3):\n",
    "    encoder_states, hidden, cell = encoder(input_seq)\n",
    "    \n",
    "    # Initialize the beams with the start token, hidden states, and a score of 0\n",
    "    beams = [(torch.tensor([[vocab.stoi[\"<SOS>\"]]]).to(device), hidden, cell, 0)]\n",
    "    completed_sentences = []\n",
    "\n",
    "    for _ in range(max_length):\n",
    "        new_beams = []\n",
    "        for seq, hidden, cell, score in beams:\n",
    "            if seq[-1] == question_vocab.stoi[\"<EOS>\"]:\n",
    "                completed_sentences.append((seq, score))\n",
    "                continue\n",
    "\n",
    "            decoder_input = seq[-1].unsqueeze(0)\n",
    "            output, hidden, cell = decoder(decoder_input, encoder_states, hidden, cell)\n",
    "            topk_probs, topk_indices = torch.topk(output, beam_width)\n",
    "\n",
    "            for i in range(beam_width):\n",
    "                new_seq = torch.cat([seq, topk_indices[0, i].unsqueeze(0)], dim=0)\n",
    "                new_score = score + torch.log(topk_probs[0, i]).item()\n",
    "                new_beams.append((new_seq, hidden, cell, new_score))\n",
    "        \n",
    "        beams = sorted(new_beams, key=lambda x: x[3], reverse=True)[:beam_width]\n",
    "    \n",
    "    if not completed_sentences:\n",
    "        completed_sentences = beams\n",
    "    \n",
    "    best_seq = max(completed_sentences, key=lambda x: x[1])[0]   \n",
    "    return [vocab.itos[idx] for idx in best_seq.squeeze().tolist() if idx not in [vocab.stoi[\"<sos>\"], vocab.stoi[\"<eos>\"]]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eaf8e9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ChatBot:\n",
    "    negative_responses = (\"no\", \"nope\", \"nah\", \"naw\", \"not a chance\", \"sorry\")\n",
    "    exit_commands = (\"quit\", \"pause\", \"exit\", \"goodbye\", \"bye\", \"later\", \"stop\")\n",
    "\n",
    "    def __init__(self, encoder, decoder, vocab):\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.vocab = vocab\n",
    "\n",
    "    def start_chat(self):\n",
    "        user_response = input(\"Hi, I'm a chatbot trained on question-answer pairs. Would you like to chat with me?\\n\")\n",
    "        if user_response.lower() in self.negative_responses:\n",
    "            print(\"Ok, have a great day!\")\n",
    "            return\n",
    "        self.chat(user_response)\n",
    "\n",
    "    def chat(self, reply):\n",
    "        while not self.make_exit(reply):\n",
    "            reply = input(self.generate_response(reply) + \"\\n\")\n",
    "\n",
    "    def preprocess_input(self, user_input):\n",
    "        tokens = [token.text.lower() for token in spacy_eng.tokenizer(user_input)]\n",
    "        numericalized_text = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_text += [self.vocab.stoi.get(token, self.vocab.stoi[\"<UNK>\"]) for token in tokens]\n",
    "        numericalized_text.append(self.vocab.stoi[\"<EOS>\"])\n",
    "        return torch.tensor(numericalized_text).unsqueeze(1).to(device)\n",
    "\n",
    "    def generate_response(self, user_input, beam_search = False):\n",
    "        input_tensor = self.preprocess_input(user_input)\n",
    "        if beam_search:\n",
    "            chatbot_response = decode_response_beam_search(self.encoder, self.decoder, input_tensor, 50, self.vocab)\n",
    "\n",
    "        chatbot_response = decode_response(self.encoder, self.decoder, input_tensor, self.vocab)\n",
    "        chatbot_response = chatbot_response.replace(\"<SOS>\", '').replace(\"<EOS>\", '')\n",
    "        return chatbot_response\n",
    "\n",
    "    def make_exit(self, reply):\n",
    "        for exit_command in self.exit_commands:\n",
    "            if exit_command in reply.lower():\n",
    "                print(\"Ok, have a great day!\")\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "chatbot = ChatBot(encoder, decoder, dataset.vocab)\n",
    "chatbot.start_chat()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
